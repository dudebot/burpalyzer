{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch_vggish_yamnet import yamnet\n",
    "from torch_vggish_yamnet.input_proc import WaveformToInput\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the waveform converter\n",
    "converter = WaveformToInput()\n",
    "\n",
    "# Initialize the YAMNet model\n",
    "model = yamnet.yamnet(pretrained=True)\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_audio(file_path, target_sr=16000):\n",
    "#     # Load audio file\n",
    "#     audio = AudioSegment.from_file(file_path, format=\"webm\")\n",
    "#     print(1 )\n",
    "#     # Resample to target sample rate\n",
    "#     audio = audio.set_frame_rate(target_sr)\n",
    "#     print(2)\n",
    "#     # If stereo, average the channels to create mono\n",
    "#     if audio.channels > 1:\n",
    "#         samples = np.array(audio.split_to_mono())\n",
    "#         averaged_samples = (samples[0].get_array_of_samples() + samples[1].get_array_of_samples()) / 2\n",
    "#     else:\n",
    "#         averaged_samples = np.array(audio.get_array_of_samples())\n",
    "#     print(3)\n",
    "#     # Convert to float32 and normalize\n",
    "#     y = averaged_samples.astype(np.float32)\n",
    "#     y /= np.iinfo(audio.array_type).max  # Normalize to [-1, 1]\n",
    "#     print(4)\n",
    "#     return y, target_sr\n",
    "\n",
    "def load_audio(file_path, target_sr=16000):\n",
    "    import torchaudio\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = torchaudio.load(file_path)  # y is a tensor of shape [channels, samples]\n",
    "\n",
    "    print(f\"Loaded audio file {file_path}, sample rate {sr}, waveform shape {y.shape}\")\n",
    "\n",
    "    # Convert to mono by averaging channels if necessary\n",
    "    if y.shape[0] > 1:\n",
    "        y = y.mean(dim=0)\n",
    "    else:\n",
    "        y = y.squeeze(0)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        y = resampler(y)\n",
    "        sr = target_sr\n",
    "\n",
    "    y = y.numpy()  # Convert tensor to numpy array if needed\n",
    "    return y, sr\n",
    "\n",
    "def convert_webm_to_wav(file_path):\n",
    "    # Define the output file path with a .wav extension\n",
    "    output_path = file_path.replace(\".webm\", \".wav\")\n",
    "    # Run the ffmpeg command to convert\n",
    "    subprocess.run(['ffmpeg', '-i', file_path, output_path, '-y'], check=True)\n",
    "    return output_path\n",
    "\n",
    "def detect_laughter(y_chunk, sr, threshold=0.5):\n",
    "    y = torch.tensor(y_chunk).float().to(device)  # Move to the correct device\n",
    "\n",
    "    # Ensure y has the correct shape [batch_size, samples]\n",
    "    if y.ndim == 1:\n",
    "        y = y.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    in_tensor = converter(y, sr).to(device)  # Move the input tensor to the same device as the model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings, logits = model(in_tensor)\n",
    "\n",
    "    probabilities = torch.softmax(logits, dim=1).cpu()  # Move to CPU for processing\n",
    "\n",
    "    class_map_path = yamnet._DEFAULT_YAMNET_CLASSES_PATH\n",
    "    with open(class_map_path, 'r') as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    laughter_indices = [i for i, name in enumerate(class_names) if 'laughter' in name.lower()]\n",
    "    if not laughter_indices:\n",
    "        raise ValueError(\"Laughter class not found in YAMNet class names.\")\n",
    "    laughter_index = laughter_indices[0]\n",
    "\n",
    "    frame_duration = 0.48\n",
    "    timestamps = np.arange(len(probabilities)) * frame_duration\n",
    "\n",
    "    laughter_probs = probabilities[:, laughter_index]\n",
    "    laughter_frames = laughter_probs > threshold\n",
    "\n",
    "    laughter_events = []\n",
    "    start_time = None\n",
    "    for i, is_laughter in enumerate(laughter_frames):\n",
    "        time = timestamps[i]\n",
    "        if is_laughter and start_time is None:\n",
    "            start_time = time\n",
    "        elif not is_laughter and start_time is not None:\n",
    "            end_time = time\n",
    "            laughter_events.append((start_time, end_time))\n",
    "            start_time = None\n",
    "    if start_time is not None:\n",
    "        laughter_events.append((start_time, timestamps[-1]))\n",
    "\n",
    "    return laughter_events\n",
    "\n",
    "def save_laughter_events(laughter_events, output_file):\n",
    "    df = pd.DataFrame(laughter_events, columns=['Start Time', 'End Time'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def process_audio_file(file_path, output_file, threshold=0.5):\n",
    "    y, sr = load_audio(file_path)\n",
    "    laughter_events = detect_laughter(y, sr, threshold=threshold)\n",
    "    save_laughter_events(laughter_events, output_file)\n",
    "    print(f\"Laughter events saved to {output_file}\")\n",
    "    \n",
    "def process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5):\n",
    "    print(\"Loading audio file...\")\n",
    "    y, sr = load_audio(file_path)\n",
    "    total_duration = len(y) / sr\n",
    "    laughter_events = []\n",
    "    total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "    print(f\"Processing audio in {chunk_duration}-second chunks...\")\n",
    "\n",
    "    for i, start in enumerate(np.arange(0, total_duration, chunk_duration)):\n",
    "        print(f\"Processing chunk {i+1} of {total_chunks}...\")\n",
    "        end = min(start + chunk_duration, total_duration)\n",
    "        start_sample = int(start * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        y_chunk = y[start_sample:end_sample]\n",
    "\n",
    "        # Process the chunk and collect laughter events\n",
    "        events = detect_laughter(y_chunk, sr, threshold)\n",
    "\n",
    "        # Adjust event times for the full audio timeline\n",
    "        adjusted_events = [(s + start, e + start) for s, e in events]\n",
    "        laughter_events.extend(adjusted_events)\n",
    "\n",
    "    # Save the aggregated laughter events\n",
    "    output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "    save_laughter_events(laughter_events, output_file)\n",
    "    print(f\"Laughter events saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = ['@StronnyCuttles/processed/ã€ASMR Streamã€‘Intense Ear Cleaning and Slime Poking!ðŸ¦‘ðŸ›ã€VAllureã€‘ [3rIax65ailI].webm']  # Replace with your list of audio files\n",
    "# for file_path in audio_files:\n",
    "#     output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "#     #process_audio_file(file_path, output_file, threshold=0.5)\n",
    "#     process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5)\n",
    "\n",
    "\n",
    "for file_path in audio_files:\n",
    "    delete_file = False\n",
    "    if file_path.endswith('.webm'):\n",
    "        file_path = convert_webm_to_wav(file_path)  # Convert if webm format\n",
    "        delete_file = True\n",
    "        \n",
    "    output_file = f\"{file_path}_laughter_timestamps.csv\"\n",
    "    #process_audio_file(file_path, output_file, threshold=0.5)\n",
    "    process_audio_in_chunks(file_path, chunk_duration=10, threshold=0.5)\n",
    "    # delete the converted file if it was created\n",
    "    if delete_file:\n",
    "        print(f\"Deleting converted file {file_path}\")\n",
    "        #os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_excessive_laughter(laughter_events, min_duration=2.0):\n",
    "    return [event for event in laughter_events if (event[1] - event[0]) >= min_duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughter_events = detect_laughter(y, sr, threshold=threshold)\n",
    "excessive_laughter_events = filter_excessive_laughter(laughter_events, min_duration=2.0)\n",
    "save_laughter_events(excessive_laughter_events, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
